{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"DQN2.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"uLIq8p7QBcz6","colab_type":"text"},"cell_type":"markdown","source":["# DQN2 for Pong\n","### Version 12.16\n","### Two Observations and Two actions"]},{"metadata":{"id":"AJ_caBjDBl2E","colab_type":"text"},"cell_type":"markdown","source":["## Prepare Environment"]},{"metadata":{"id":"k8s8KaYMBvuz","colab_type":"text"},"cell_type":"markdown","source":["Please uncomment and run this chuck if you have not installed gym."]},{"metadata":{"id":"4iG4ttJzC1AV","colab_type":"code","outputId":"904f6a02-bf58-4c3e-dbd6-151adbce19c5","colab":{"base_uri":"https://localhost:8080/","height":261}},"cell_type":"code","source":["# install OpenAI gym per https://gym.openai.com/docs/\n","#!pip install gym\n","#!pip install gym[atari]\n","#!apt-get install python-opengl"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: gym in /usr/local/lib/python3.6/dist-packages (0.10.9)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym) (1.1.0)\n","Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym) (1.14.6)\n","Requirement already satisfied: requests>=2.0 in /usr/local/lib/python3.6/dist-packages (from gym) (2.18.4)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gym) (1.11.0)\n","Requirement already satisfied: pyglet>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym) (1.3.2)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym) (2018.11.29)\n","Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym) (2.6)\n","Requirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym) (1.22)\n","Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym) (3.0.4)\n","Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet>=1.2.0->gym) (0.16.0)\n"],"name":"stdout"}]},{"metadata":{"id":"Q7CAbcWdDvHS","colab_type":"text"},"cell_type":"markdown","source":["### Import neccessary modules and make environment"]},{"metadata":{"id":"KhXxds0_CZ9Y","colab_type":"code","colab":{}},"cell_type":"code","source":["import gym\n","import random\n","import numpy as np\n","import pandas as pd\n","import tensorflow as tf\n","env = gym.make('Pong-v0')\n","\n","import os\n","LOG_DIR = './tmp_DQN2'\n","if not os.path.exists(LOG_DIR):\n","  os.makedirs(LOG_DIR)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"eBD2hYmgDdDl","colab_type":"text"},"cell_type":"markdown","source":["### Create Memory Buffer"]},{"metadata":{"id":"gPk4dxW2Da5X","colab_type":"code","colab":{}},"cell_type":"code","source":["# Modified from code which professor provided in scratch06\n","class Replay:\n","  # accepts a tuple (s,a,r,s') and keeps a list, returns a random batch of tuples as needed\n","  def __init__(self):\n","    self.buffer = []\n","    self.length = 0\n","    self.max_length = 300000\n","\n","  def write(self, data):\n","    if self.length >= self.max_length:\n","        # drop oldest data point to make room for new\n","        self.buffer.pop(0)\n","        self.length -= 1\n","    self.buffer.append(data)\n","    self.length += 1\n","\n","\n","  def read(self, batch_size):\n","    # randomly sample a batch and return a list of them\n","    return random.sample(self.buffer,min(batch_size,self.length))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"eiRQY5VuD7tb","colab_type":"text"},"cell_type":"markdown","source":["### Network to abstract all tensorflow away from agent"]},{"metadata":{"id":"k_iV__-9EbqU","colab_type":"code","colab":{}},"cell_type":"code","source":["# Based on https://github.com/fg91/Deep-Q-Learning/blob/master/DQN.ipynb\n","class Network:\n","  def __init__(self, session, hidden=64, learning_rate = 0.00025):\n","    '''\n","    Args:\n","        hidden: Integer, Number of filters in the final convolutional layer. \n","                This is different from the DeepMind implementation\n","        learning_rate: Float, Learning rate for the Adam optimizer\n","    '''\n","    self.session = session\n","    self.n_actions = 2 # number of possible actions \n","    self.hidden = hidden\n","    self.learning_rate = learning_rate\n","    self.frame_height = 80 # Height of a preprocessed frame of Pong\n","    self.frame_width = 80 # Width of a preprocessed frame of Pong\n","    self.y = tf.placeholder(tf.float32, [None, self.n_actions])\n","    self.observations = tf.placeholder(shape=[None, self.frame_height * self.frame_width], \n","                                dtype=tf.float32)\n","    self.input = tf.reshape(self.observations, [tf.shape(self.observations)[0],80,80,1])\n","\n","\n","    # Convolutional layers\n","    self.conv1 = tf.layers.conv2d(\n","        inputs=self.input, filters=32, kernel_size=[8, 8], strides=4,\n","        kernel_initializer=tf.variance_scaling_initializer(scale=2),\n","        padding=\"valid\", activation=tf.nn.relu, use_bias=False, name='conv1')\n","    self.conv2 = tf.layers.conv2d(\n","        inputs=self.conv1, filters=64, kernel_size=[4, 4], strides=2, \n","        kernel_initializer=tf.variance_scaling_initializer(scale=2),\n","        padding=\"valid\", activation=tf.nn.relu, use_bias=False, name='conv2')\n","    self.conv3 = tf.layers.conv2d(\n","        inputs=self.conv2, filters=64, kernel_size=[3, 3], strides=1, \n","        kernel_initializer=tf.variance_scaling_initializer(scale=2),\n","        padding=\"valid\", activation=tf.nn.relu, use_bias=False, name='conv3')\n","    self.conv4 = tf.layers.conv2d(\n","        inputs=self.conv3, filters=hidden, kernel_size=[3, 3], strides=1, \n","        kernel_initializer=tf.variance_scaling_initializer(scale=2),\n","        padding=\"valid\", activation=tf.nn.relu, use_bias=False, name='conv4')\n","\n","    self.q_values = tf.layers.dense(\n","        inputs=tf.layers.flatten(self.conv4), units=self.n_actions,\n","        kernel_initializer=tf.variance_scaling_initializer(scale=2), name=\"q_values\")\n","    self.best_action = tf.argmax(self.q_values, 1)\n","\n","    # loss, train_step\n","    self.loss = tf.reduce_sum(tf.square(self.y - self.q_values),1)\n","    self.train_step = tf.train.AdamOptimizer(learning_rate).minimize(self.loss)\n","\n","  def compute(self, state):\n","    # evaluate the network and return the action values [q(s,a=0),q(s,a=1)]\n","    return self.session.run(self.q_values, feed_dict={self.observations: state})\n","\n","  def get_best_action(self, state):\n","    return self.session.run(self.best_action, feed_dict={self.observations: state})\n","\n","\n","  def train(self, x_batch, y_batch):\n","    # take a training step\n","    _ = self.session.run(self.train_step, feed_dict={self.observations: x_batch, self.y: y_batch})"],"execution_count":0,"outputs":[]},{"metadata":{"id":"gdJotFS0GF_9","colab_type":"text"},"cell_type":"markdown","source":["### Pong deep q network agent"]},{"metadata":{"id":"VBCyUjPbDBkx","colab_type":"code","colab":{}},"cell_type":"code","source":["# Modified from code which professor provided in scratch06\n","class Agent: \n","\n","  def __init__(self, tf_session):\n","      self.n_actions = 2\n","      self.frame_height = 80\n","      self.frame_width = 80\n","      # first what reward has the agent accrued so far\n","      self.total_reward = 0 \n","      # discount, exploration rates, batch size\n","      self.gamma = 0.99\n","      self.epsilon = 1.0\n","      self.batch_size = 32\n","      # make an experience replay buffer\n","      self.replay_buffer = Replay()\n","      # make the network that will be the q function\n","      self.q = Network(tf_session)  \n","      self.sess = tf_session\n","      self.checkpoint_file = os.path.join('./tmp_DQN2', 'deep_q_network.ckpt')\n","      self.saver = tf.train.Saver()\n","\n","\n","  def gather_experience(self, state, action, reward, new_state):\n","      # push this experience onto the replay buffer\n","      self.replay_buffer.write(( state, action, reward, new_state))\n","\n","  def choose_action(self, state):\n","      # behave according to an epsilon greedy policy\n","      if np.random.rand(1) < self.epsilon:\n","          return np.random.randint(0, self.n_actions)\n","      else:\n","          return self.q.get_best_action(state) [0] \n","\n","\n","  def q_update(self, step):\n","      # pull a batch from the buffer\n","      sars_batch = self.replay_buffer.read(self.batch_size)\n","      # compute the q function for all last_state and state\n","      q_last = self.q.compute([s[0] for s in sars_batch])\n","      # q_next for current state requires a bit more attention, since done flag means q should be zero\n","      q_this = np.zeros_like(q_last) # initialize q to zeros\n","      ind_not_none = [i for i in range(np.shape(sars_batch)[0]) if sars_batch[i][3] is not None]\n","\n","      q_this_not_none = self.q.compute([sb[3] for sb in sars_batch if sb[3] is not None])\n","\n","      # now fill q_this with just the valid q, leaving others [0,0]\n","      for i in range(len(ind_not_none)):\n","          q_this[ind_not_none[i],:] = q_this_not_none[i,:]\n","\n","      # now chunk this up as the train_step expects\n","      x_batch = np.zeros([np.shape(sars_batch)[0], self.frame_height * self.frame_width])\n","      y_batch = np.zeros([np.shape(sars_batch)[0], self.n_actions])\n","      for i in range(np.shape(sars_batch)[0]):\n","          x_batch[i,:] = sars_batch[i][0]\n","          for j in range(2):\n","              if j == sars_batch[i][1]:\n","                  # the key step... this is the q learning target\n","                  y_batch[i,j] = sars_batch[i][2] + self.gamma*np.max(q_this[i])\n","              else:\n","                  y_batch[i,j] = q_last[i][j]\n","\n","      # now run the train step\n","      self.q.train(x_batch,y_batch)\n","\n","  def set_epsilon(self, step):\n","      # set epsilon = 1 when evaluation\n","      if step > 300000 and step <1000000: \n","          self.epsilon = max(0.1, self.epsilon - ((1-0.1)/(1000000-300000)))\n","      if step >= 1000000: \n","          self.epsilon = max(0.01, self.epsilon - ((0.1-0.01)/ (10**6)))\n","\n","  def reset_epsilon(self):\n","      # reset method for running greedy after training\n","      self.epsilon = 0.0\n","\n","  def gather_reward(self, reward):\n","      self.total_reward += reward\n","\n","  def get_total_reward(self):\n","      return self.total_reward\n","\n","  def set_total_reward(self, new_total):\n","      self.total_reward = new_total\n","\n","  def load_checkpoint(self):\n","      print(\"Loading checkpoint...\")\n","      self.saver.restore(self.sess, self.checkpoint_file)\n","\n","  def save_checkpoint(self):\n","      print(\"Saving checkpoint...\")\n","      self.saver.save(self.sess, self.checkpoint_file)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"QnS6S_8eHtXO","colab_type":"text"},"cell_type":"markdown","source":["### Preprocess frames"]},{"metadata":{"id":"9AfzM0W4HswV","colab_type":"code","colab":{}},"cell_type":"code","source":["# From Andrej's code\n","def prepro(I):\n","  # prepro 210x160x3 uint8 frame into 6400 (80x80) 1D float vector\n","  I = I[35:195]  # crop\n","  I = I[::2, ::2, 0]  # downsample by factor of 2\n","  I[I == 144] = 0  # erase background (background type 1)\n","  I[I == 109] = 0  # erase background (background type 2)\n","  I[I != 0] = 1  # everything else (paddles, ball) just set to 1\n","  return I.astype(np.float).ravel()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"A1Z5VymAJSwn","colab_type":"text"},"cell_type":"markdown","source":["### Run it..."]},{"metadata":{"id":"GFFGJiTeIvll","colab_type":"code","colab":{}},"cell_type":"code","source":["# map actions from 0, 1 to 2, 3 to match the environment's response\n","action_map = {0:2, 1:3}\n","\n","with tf.Graph().as_default():\n","    load_checkpoint = False # \n","    ep_rewards = []\n","    ep_smooth_rewards = []\n","    smoothed_reward = None\n","    with tf.Session() as sess:\n","        # create an agent\n","        agent = Agent(sess)\n","        if load_checkpoint:\n","            agent.load_checkpoint()\n","        # usual tf initialization\n","        sess.run(tf.global_variables_initializer())      \n","        \n","        #################################\n","        ## Q-learn (train) DQN on Pong ##\n","        #################################\n","        episode_n = 0\n","        n_steps = 1\n","        \n","        while True: \n","          episode_n += 1\n","          # reset environment and agent\n","          observation1 = env.reset()\n","          # random pick first 3 action and record frame (observation)\n","          action = action_map[np.random.randint(0, 2)]\n","          observation2, _, _, _ = env.step(action)\n","\n","          observation1 = prepro(observation1)\n","          observation2 = prepro(observation2)\n","\n","          agent.set_total_reward(0)\n","\n","          episode_done = False\n","          round_n = 1\n","\n","          while not episode_done:\n","            # use the difference of two consecutive frames(observations) as a state\n","            last_state = observation2 - observation1\n","            # agent chooses an action\n","            action01 = agent.choose_action([last_state])\n","            action = action_map[action01]\n","            # agent takes the action, and the environment responds\n","            observation3, reward, episode_done, info = env.step(action)\n","            observation3 = prepro(observation3)\n","            # update rates\n","            agent.set_epsilon(n_steps)\n","            n_steps += 1\n","            # update agent with reward and data\n","            agent.gather_reward(reward)\n","            # update state\n","            state = observation3 - observation2\n","            observation1 = observation2\n","            observation2 = observation3\n","\n","            if episode_done: \n","                agent.gather_experience(last_state, action01, reward, None)\n","            else:\n","                agent.gather_experience(last_state, action01, reward, state)\n","\n","            if n_steps > 300000:\n","                agent.q_update(n_steps)\n","\n","            if reward != 0:\n","                round_n += 1 \n","\n","          print(\"Episode %d finished after %d rounds\" % (episode_n, round_n))\n","          print('Step is ', n_steps, ', epsilon is', agent.epsilon)\n","\n","          episode_reward_sum = agent.get_total_reward()\n","          # exponentially smoothed version of reward\n","          if smoothed_reward is None:\n","            smoothed_reward = episode_reward_sum\n","          else:\n","            smoothed_reward = smoothed_reward * 0.99 + episode_reward_sum * 0.01\n","\n","          print(\"Reward total was %.3f; discounted moving average of reward is %.3f\" % (episode_reward_sum, smoothed_reward))\n","\n","          ep_rewards.append(episode_reward_sum)\n","          ep_smooth_rewards.append(smoothed_reward)\n","\n","\n","          # Set termination rule\n","          if smoothed_reward > 5:\n","            break\n","\n","          if episode_n % 20 == 0:\n","            agent.save_checkpoint()\n","            load_checkpoint = True"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Mjjd6uNdLs8q","colab_type":"text"},"cell_type":"markdown","source":["### Write out rewards and smoothed rewards to csv file"]},{"metadata":{"id":"J81bc_pX6Bqc","colab_type":"code","colab":{}},"cell_type":"code","source":["df = pd.DataFrame(ep_rewards, columns=[\"ep_rewards\"])\n","df.to_csv('ep_rewards.csv', index=False)\n","\n","df = pd.DataFrame(smoothed_reward, columns=[\"smoothed_reward\"])\n","df.to_csv('smoothed_reward.csv', index=False)"],"execution_count":0,"outputs":[]}]}