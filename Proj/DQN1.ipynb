{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"DQN1.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"uLIq8p7QBcz6","colab_type":"text"},"cell_type":"markdown","source":["# DQN1 for Pong\n","### Version 12.14\n","### Four Observations and Six actions"]},{"metadata":{"id":"AJ_caBjDBl2E","colab_type":"text"},"cell_type":"markdown","source":["## Prepare Environment"]},{"metadata":{"id":"k8s8KaYMBvuz","colab_type":"text"},"cell_type":"markdown","source":["Please uncomment and run this chuck if you have not installed gym."]},{"metadata":{"id":"4iG4ttJzC1AV","colab_type":"code","outputId":"fd682770-306e-418a-a1d5-a64f3b50a5e8","executionInfo":{"status":"ok","timestamp":1545070175612,"user_tz":300,"elapsed":17269,"user":{"displayName":"Wenshan Wang","photoUrl":"","userId":"01906098267717982602"}},"colab":{"base_uri":"https://localhost:8080/","height":760}},"cell_type":"code","source":["# install OpenAI gym per https://gym.openai.com/docs/\n","#!pip install gym\n","#!pip install gym[atari]\n","#!apt-get install python-opengl"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: gym in /usr/local/lib/python3.6/dist-packages (0.10.9)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym) (1.1.0)\n","Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym) (1.14.6)\n","Requirement already satisfied: requests>=2.0 in /usr/local/lib/python3.6/dist-packages (from gym) (2.18.4)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gym) (1.11.0)\n","Requirement already satisfied: pyglet>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym) (1.3.2)\n","Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym) (3.0.4)\n","Requirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym) (1.22)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym) (2018.11.29)\n","Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym) (2.6)\n","Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet>=1.2.0->gym) (0.16.0)\n","Requirement already satisfied: gym[atari] in /usr/local/lib/python3.6/dist-packages (0.10.9)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (1.1.0)\n","Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (1.14.6)\n","Requirement already satisfied: requests>=2.0 in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (2.18.4)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (1.11.0)\n","Requirement already satisfied: pyglet>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (1.3.2)\n","Requirement already satisfied: atari_py>=0.1.4 in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (0.1.7)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (4.0.0)\n","Requirement already satisfied: PyOpenGL in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (3.1.0)\n","Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym[atari]) (2.6)\n","Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym[atari]) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym[atari]) (2018.11.29)\n","Requirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym[atari]) (1.22)\n","Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet>=1.2.0->gym[atari]) (0.16.0)\n","Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from Pillow->gym[atari]) (0.46)\n","Reading package lists... Done\n","Building dependency tree       \n","Reading state information... Done\n","Suggested packages:\n","  libgle3\n","The following NEW packages will be installed:\n","  python-opengl\n","0 upgraded, 1 newly installed, 0 to remove and 8 not upgraded.\n","Need to get 496 kB of archives.\n","After this operation, 5,416 kB of additional disk space will be used.\n","Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 python-opengl all 3.1.0+dfsg-1 [496 kB]\n","Fetched 496 kB in 2s (264 kB/s)\n","Selecting previously unselected package python-opengl.\n","(Reading database ... 110377 files and directories currently installed.)\n","Preparing to unpack .../python-opengl_3.1.0+dfsg-1_all.deb ...\n","Unpacking python-opengl (3.1.0+dfsg-1) ...\n","Setting up python-opengl (3.1.0+dfsg-1) ...\n"],"name":"stdout"}]},{"metadata":{"id":"Q7CAbcWdDvHS","colab_type":"text"},"cell_type":"markdown","source":["### Import neccessary modules and make environment"]},{"metadata":{"id":"KhXxds0_CZ9Y","colab_type":"code","outputId":"441b1a0d-ddc0-431d-ff0c-b41011012931","executionInfo":{"status":"ok","timestamp":1545070179733,"user_tz":300,"elapsed":1708,"user":{"displayName":"Wenshan Wang","photoUrl":"","userId":"01906098267717982602"}},"colab":{"base_uri":"https://localhost:8080/","height":72}},"cell_type":"code","source":["import gym\n","import random\n","import numpy as np\n","import pandas as pd\n","import tensorflow as tf\n","env = gym.make('Pong-v0')\n","\n","import os\n","LOG_DIR = './tmp_DQN1'\n","if not os.path.exists(LOG_DIR):\n","  os.makedirs(LOG_DIR)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n","  result = entry_point.load(False)\n"],"name":"stderr"}]},{"metadata":{"id":"eBD2hYmgDdDl","colab_type":"text"},"cell_type":"markdown","source":["### Create Memory Buffer"]},{"metadata":{"id":"gPk4dxW2Da5X","colab_type":"code","colab":{}},"cell_type":"code","source":["# Modified from code which professor provided in scratch06\n","class Replay:\n","  # accepts a tuple (s,a,r,s') and keeps a list, returns a random batch of tuples as needed\n","  def __init__(self):\n","    self.buffer = []\n","    self.length = 0\n","    self.max_length = 0.8*10**5\n","\n","  def write(self, data):\n","    if self.length >= self.max_length:\n","        # drop oldest data point to make room for new\n","        self.buffer.pop(0)\n","        self.length -= 1\n","    self.buffer.append(data)\n","    self.length += 1\n","\n","\n","  def read(self, batch_size):\n","    # randomly sample a batch and return a list of them\n","    return random.sample(self.buffer,min(batch_size,self.length))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"eiRQY5VuD7tb","colab_type":"text"},"cell_type":"markdown","source":["### Network to abstract all tensorflow away from agent"]},{"metadata":{"id":"k_iV__-9EbqU","colab_type":"code","colab":{}},"cell_type":"code","source":["# Based on https://github.com/fg91/Deep-Q-Learning/blob/master/DQN.ipynb\n","class Network:\n","  def __init__(self, session, hidden=64, learning_rate = 0.00025):\n","    '''\n","    Args:\n","        hidden: Integer, Number of filters in the final convolutional layer. \n","                This is different from the DeepMind implementation\n","        learning_rate: Float, Learning rate for the Adam optimizer\n","    '''\n","    self.session = session\n","    self.n_actions = env.action_space.n # number of possible actions \n","    self.hidden = hidden\n","    self.learning_rate = learning_rate\n","    self.frame_height = 80 # Height of a preprocessed frame of Pong\n","    self.frame_width = 80 # Width of a preprocessed frame of Pong\n","    self.y = tf.placeholder(tf.float32, [None, self.n_actions])\n","    self.observations = tf.placeholder(shape=[None, self.frame_height * self.frame_width, 4], \n","                                dtype=tf.float32)\n","    self.input = tf.reshape(self.observations, [tf.shape(self.observations)[0],80,80,4])\n","\n","\n","    # Convolutional layers\n","    self.conv1 = tf.layers.conv2d(\n","        inputs=self.input, filters=32, kernel_size=[8, 8], strides=4,\n","        kernel_initializer=tf.variance_scaling_initializer(scale=2),\n","        padding=\"valid\", activation=tf.nn.relu, use_bias=False, name='conv1')\n","    self.conv2 = tf.layers.conv2d(\n","        inputs=self.conv1, filters=64, kernel_size=[4, 4], strides=2, \n","        kernel_initializer=tf.variance_scaling_initializer(scale=2),\n","        padding=\"valid\", activation=tf.nn.relu, use_bias=False, name='conv2')\n","    self.conv3 = tf.layers.conv2d(\n","        inputs=self.conv2, filters=64, kernel_size=[3, 3], strides=1, \n","        kernel_initializer=tf.variance_scaling_initializer(scale=2),\n","        padding=\"valid\", activation=tf.nn.relu, use_bias=False, name='conv3')\n","    self.conv4 = tf.layers.conv2d(\n","        inputs=self.conv3, filters=hidden, kernel_size=[3, 3], strides=1, \n","        kernel_initializer=tf.variance_scaling_initializer(scale=2),\n","        padding=\"valid\", activation=tf.nn.relu, use_bias=False, name='conv4')\n","\n","    self.q_values = tf.layers.dense(\n","        inputs=tf.layers.flatten(self.conv4), units=self.n_actions,\n","        kernel_initializer=tf.variance_scaling_initializer(scale=2), name=\"q_values\")\n","    self.best_action = tf.argmax(self.q_values, 1)\n","\n","    # loss, train_step\n","    self.loss = tf.reduce_sum(tf.square(self.y - self.q_values),1)\n","    self.train_step = tf.train.AdamOptimizer(learning_rate).minimize(self.loss)\n","\n","  def compute(self, state):\n","    # evaluate the network and return the action values [q(s,a=0),q(s,a=1)]\n","    return self.session.run(self.q_values, feed_dict={self.observations: state})\n","\n","  def get_best_action(self, state):\n","    return self.session.run(self.best_action, feed_dict={self.observations: state})\n","\n","\n","  def train(self, x_batch, y_batch):\n","    # take a training step\n","    _ = self.session.run(self.train_step, feed_dict={self.observations: x_batch, self.y: y_batch})"],"execution_count":0,"outputs":[]},{"metadata":{"id":"gdJotFS0GF_9","colab_type":"text"},"cell_type":"markdown","source":["### Pong deep q network agent"]},{"metadata":{"id":"VBCyUjPbDBkx","colab_type":"code","colab":{}},"cell_type":"code","source":["# Modified from code which professor provided in scratch06\n","class Agent: \n","\n","  def __init__(self, tf_session):\n","      self.n_actions = env.action_space.n\n","      self.frame_height = 80\n","      self.frame_width = 80\n","      self.agent_history_length = 4\n","      # first what reward has the agent accrued so far\n","      self.total_reward = 0 \n","      # discount, exploration rates, batch size\n","      self.gamma = 0.99\n","      self.epsilon = 1.0\n","      self.batch_size = 32\n","      # make an experience replay buffer\n","      self.replay_buffer = Replay()\n","      # make the network that will be the q function\n","      self.q = Network(tf_session)  \n","      self.sess = tf_session\n","      self.checkpoint_file = os.path.join('./tmp_DQN1', 'deep_q_network.ckpt')\n","      self.saver = tf.train.Saver()\n","\n","\n","  def gather_experience(self, state, action, reward, new_state):\n","      # push this experience onto the replay buffer\n","      self.replay_buffer.write(( state, action, reward, new_state))\n","\n","  def choose_action(self, state):\n","      # behave according to an epsilon greedy policy\n","      if np.random.rand(1) < self.epsilon:\n","          return np.random.randint(0, self.n_actions)\n","      else:\n","          return self.q.get_best_action(state) [0] \n","\n","\n","  def q_update(self, step):\n","      # pull a batch from the buffer\n","      sars_batch = self.replay_buffer.read(self.batch_size)\n","      # compute the q function for all last_state and state\n","      q_last = self.q.compute([s[0] for s in sars_batch])\n","      # q_next for current state requires a bit more attention, since done flag means q should be zero\n","      q_this = np.zeros_like(q_last) # initialize q to zeros\n","      ind_not_none = [i for i in range(np.shape(sars_batch)[0]) if sars_batch[i][3] is not None]\n","\n","      q_this_not_none = self.q.compute([sb[3] for sb in sars_batch if sb[3] is not None])\n","\n","      # now fill q_this with just the valid q, leaving others [0,0]\n","      for i in range(len(ind_not_none)):\n","          q_this[ind_not_none[i],:] = q_this_not_none[i,:]\n","\n","      # now chunk this up as the train_step expects\n","      x_batch = np.zeros([np.shape(sars_batch)[0], self.frame_height * self.frame_width])\n","      y_batch = np.zeros([np.shape(sars_batch)[0], self.n_actions])\n","      for i in range(np.shape(sars_batch)[0]):\n","          x_batch[i,:] = sars_batch[i][0]\n","          for j in range(6):\n","              if j == sars_batch[i][1]:\n","                  # the key step... this is the q learning target\n","                  y_batch[i,j] = sars_batch[i][2] + self.gamma*np.max(q_this[i])\n","              else:\n","                  y_batch[i,j] = q_last[i][j]\n","\n","      # now run the train step\n","      self.q.train(x_batch,y_batch)\n","\n","  def set_epsilon(self, step):\n","      # set epsilon = 1 when evaluation\n","      if step > 80000: \n","          self.epsilon = max(0.1, self.epsilon - (1. / (0.8 * 10. ** 6.)))\n","\n","  def reset_epsilon(self):\n","      # reset method for running greedy after training\n","      self.epsilon = 0.0\n","\n","  def gather_reward(self, reward):\n","      self.total_reward += reward\n","\n","  def get_total_reward(self):\n","      return self.total_reward\n","\n","  def set_total_reward(self, new_total):\n","      self.total_reward = new_total\n","\n","  def load_checkpoint(self):\n","      print(\"Loading checkpoint...\")\n","      self.saver.restore(self.sess, self.checkpoint_file)\n","\n","  def save_checkpoint(self):\n","      print(\"Saving checkpoint...\")\n","      self.saver.save(self.sess, self.checkpoint_file)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"QnS6S_8eHtXO","colab_type":"text"},"cell_type":"markdown","source":["### Preprocess frames"]},{"metadata":{"id":"9AfzM0W4HswV","colab_type":"code","colab":{}},"cell_type":"code","source":["# From Andrej's code\n","def prepro(I):\n","  # prepro 210x160x3 uint8 frame into 6400 (80x80) 1D float vector\n","  I = I[35:195]  # crop\n","  I = I[::2, ::2, 0]  # downsample by factor of 2\n","  I[I == 144] = 0  # erase background (background type 1)\n","  I[I == 109] = 0  # erase background (background type 2)\n","  I[I != 0] = 1  # everything else (paddles, ball) just set to 1\n","  return I.astype(np.float).ravel()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"A1Z5VymAJSwn","colab_type":"text"},"cell_type":"markdown","source":["### Run it..."]},{"metadata":{"id":"GFFGJiTeIvll","colab_type":"code","outputId":"a623a5f4-295b-4a43-9642-eaa283ceab65","executionInfo":{"status":"error","timestamp":1545069163334,"user_tz":300,"elapsed":27622,"user":{"displayName":"Xiaoxiao Guo","photoUrl":"","userId":"14809025470141546301"}},"colab":{"base_uri":"https://localhost:8080/","height":1226}},"cell_type":"code","source":["with tf.Graph().as_default():\n","    load_checkpoint = False # change to True when using checkpoint\n","    ep_rewards = []\n","    ep_smooth_rewards = []\n","    smoothed_reward = None\n","    with tf.Session() as sess:\n","        # create an agent\n","        agent = Agent(sess)\n","        if load_checkpoint:\n","            agent.load_checkpoint()\n","        # usual tf initialization\n","        sess.run(tf.global_variables_initializer())      \n","        \n","        #################################\n","        ## Q-learn (train) DQN on Pong ##\n","        #################################\n","        episode_n = 0\n","        n_steps = 1\n","        \n","        while True: \n","          episode_n += 1\n","          # reset environment and agent\n","          observation1 = env.reset()\n","          # random pick first 3 action and record frame (observation)\n","          observation2, _, _, _ = env.step(np.random.randint(0, env.action_space.n))\n","          observation3, _, _, _ = env.step(np.random.randint(0, env.action_space.n))\n","          observation4, _, _, _ = env.step(np.random.randint(0, env.action_space.n))\n","          observation1 = prepro(observation1)\n","          observation2 = prepro(observation2)\n","          observation3 = prepro(observation3)\n","          observation4 = prepro(observation4)\n","\n","          state = [observation1, observation2, observation3, observation4]\n","            \n","          agent.set_total_reward(0)\n","\n","          episode_done = False\n","          round_n = 1\n","\n","          while not episode_done:\n","            last_state = state\n","            # agent chooses an action\n","            action = agent.choose_action([np.array(last_state).transpose()])\n","            # agent takes the action, and the environment responds\n","            observation, reward, episode_done, info = env.step(action)\n","            observation = prepro(observation)\n","            # update rates\n","            agent.set_epsilon(n_steps)\n","            n_steps += 1\n","            # update agent with reward and data\n","            agent.gather_reward(reward)\n","            # update state\n","            state[0] = last_state[1]\n","            state[1] = last_state[2]\n","            state[2] = last_state[3]\n","            state[3] = observation\n","\n","            if episode_done: \n","                agent.gather_experience(np.array(last_state).transpose(), action, reward, None)\n","            else:\n","                agent.gather_experience(np.array(last_state).transpose(), action, reward, np.array(state).transpose())\n","\n","            if n_steps > 80000:\n","                agent.q_update(n_steps)\n","\n","            if reward != 0:\n","                round_n += 1 \n","\n","          print(\"Episode %d finished after %d rounds\" % (episode_n, round_n))\n","          print('Step is ', n_steps, ', epsilon is', agent.epsilon)\n","\n","          episode_reward_sum = agent.get_total_reward()\n","          # exponentially smoothed version of reward\n","          if smoothed_reward is None:\n","            smoothed_reward = episode_reward_sum\n","          else:\n","            smoothed_reward = smoothed_reward * 0.99 + episode_reward_sum * 0.01\n","\n","          print(\"Reward total was %.3f; discounted moving average of reward is %.3f\" % (episode_reward_sum, smoothed_reward))\n","\n","          ep_rewards.append(episode_reward_sum)\n","          ep_smooth_rewards.append(smoothed_reward)\n","\n","\n","          # Set termination rule\n","          if smoothed_reward > 5:\n","            break\n","\n","          if episode_n % 20 == 0:\n","            agent.save_checkpoint()\n","            load_checkpoint = True"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Episode 1 finished after 22 rounds\n","Step is  1015 , epsilon is 1.0\n","Reward total was -21.000; discounted moving average of reward is -21.000\n","Episode 2 finished after 23 rounds\n","Step is  2402 , epsilon is 1.0\n","Reward total was -20.000; discounted moving average of reward is -20.990\n","Episode 3 finished after 24 rounds\n","Step is  3644 , epsilon is 1.0\n","Reward total was -19.000; discounted moving average of reward is -20.970\n","Episode 4 finished after 24 rounds\n","Step is  4943 , epsilon is 1.0\n","Reward total was -19.000; discounted moving average of reward is -20.950\n","Episode 5 finished after 22 rounds\n","Step is  6004 , epsilon is 1.0\n","Reward total was -21.000; discounted moving average of reward is -20.951\n","Episode 6 finished after 22 rounds\n","Step is  7454 , epsilon is 1.0\n","Reward total was -21.000; discounted moving average of reward is -20.951\n","Episode 7 finished after 22 rounds\n","Step is  8569 , epsilon is 1.0\n","Reward total was -21.000; discounted moving average of reward is -20.952\n","Episode 8 finished after 24 rounds\n","Step is  10163 , epsilon is 1.0\n","Reward total was -19.000; discounted moving average of reward is -20.932\n","Episode 9 finished after 23 rounds\n","Step is  11641 , epsilon is 1.0\n","Reward total was -20.000; discounted moving average of reward is -20.923\n","Episode 10 finished after 22 rounds\n","Step is  12841 , epsilon is 1.0\n","Reward total was -21.000; discounted moving average of reward is -20.924\n","Episode 11 finished after 23 rounds\n","Step is  14046 , epsilon is 1.0\n","Reward total was -20.000; discounted moving average of reward is -20.915\n","Episode 12 finished after 22 rounds\n","Step is  15213 , epsilon is 1.0\n","Reward total was -21.000; discounted moving average of reward is -20.915\n","Episode 13 finished after 24 rounds\n","Step is  16478 , epsilon is 1.0\n","Reward total was -19.000; discounted moving average of reward is -20.896\n","Episode 14 finished after 23 rounds\n","Step is  17855 , epsilon is 1.0\n","Reward total was -20.000; discounted moving average of reward is -20.887\n","Episode 15 finished after 22 rounds\n","Step is  19071 , epsilon is 1.0\n","Reward total was -21.000; discounted moving average of reward is -20.888\n","Episode 16 finished after 22 rounds\n","Step is  20205 , epsilon is 1.0\n","Reward total was -21.000; discounted moving average of reward is -20.890\n","Episode 17 finished after 23 rounds\n","Step is  21567 , epsilon is 1.0\n","Reward total was -20.000; discounted moving average of reward is -20.881\n","Episode 18 finished after 22 rounds\n","Step is  22579 , epsilon is 1.0\n","Reward total was -21.000; discounted moving average of reward is -20.882\n","Episode 19 finished after 24 rounds\n","Step is  23854 , epsilon is 1.0\n","Reward total was -19.000; discounted moving average of reward is -20.863\n","Episode 20 finished after 22 rounds\n","Step is  24946 , epsilon is 1.0\n","Reward total was -21.000; discounted moving average of reward is -20.864\n","Saving checkpoint...\n","Episode 21 finished after 25 rounds\n","Step is  26300 , epsilon is 1.0\n","Reward total was -18.000; discounted moving average of reward is -20.836\n","Episode 22 finished after 23 rounds\n","Step is  27555 , epsilon is 1.0\n","Reward total was -20.000; discounted moving average of reward is -20.827\n","Episode 23 finished after 22 rounds\n","Step is  28642 , epsilon is 1.0\n","Reward total was -21.000; discounted moving average of reward is -20.829\n"],"name":"stdout"}]},{"metadata":{"id":"Mjjd6uNdLs8q","colab_type":"text"},"cell_type":"markdown","source":["### Write out rewards and smoothed rewards to csv file"]},{"metadata":{"id":"J81bc_pX6Bqc","colab_type":"code","colab":{}},"cell_type":"code","source":["df = pd.DataFrame(ep_rewards, columns=[\"ep_rewards\"])\n","df.to_csv('ep_rewards.csv', index=False)\n","\n","df = pd.DataFrame(smoothed_reward, columns=[\"smoothed_reward\"])\n","df.to_csv('smoothed_reward.csv', index=False)"],"execution_count":0,"outputs":[]}]}