{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"PG.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"metadata":{"id":"iH4ISrapRCoI","colab_type":"text"},"cell_type":"markdown","source":["# PG for Pong \n","### Version 12.05"]},{"metadata":{"id":"NrTClOugRCjb","colab_type":"text"},"cell_type":"markdown","source":["## Prepare Environment\n","Please uncomment and run this chuck if you have not installed gym."]},{"metadata":{"id":"IWU4iYN_OoVS","colab_type":"code","colab":{}},"cell_type":"code","source":["#!pip install gym\n","#!pip install gym[atari]\n","#!apt-get install python-opengl"],"execution_count":0,"outputs":[]},{"metadata":{"id":"jHB8LqhrRmIw","colab_type":"text"},"cell_type":"markdown","source":["## Import neccessary modules and make directory"]},{"metadata":{"id":"daQIcIS6R65H","colab_type":"code","colab":{}},"cell_type":"code","source":["import os\n","import os.path\n","\n","# Ensure target log dir exists\n","LOG_DIR = './tmp_PG'\n","if not os.path.exists(LOG_DIR):\n","    os.makedirs(LOG_DIR)\n","    \n","import numpy as np\n","import tensorflow as tf\n","\n","import argparse\n","import pickle\n","import numpy as np\n","import gym\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"JeUoqBMwSPGN","colab_type":"text"},"cell_type":"markdown","source":["## Policy Gradient Network"]},{"metadata":{"id":"vn82TupOSZ4-","colab_type":"code","colab":{}},"cell_type":"code","source":["OBSERVATIONS_SIZE = 6400\n","\n","\n","class Network:\n","    def __init__(self, learning_rate, checkpoints_dir):\n","      \n","        self.learning_rate = learning_rate\n","\n","        self.observations = tf.placeholder(tf.float32, [None, OBSERVATIONS_SIZE])\n","                \n","        self.sampled_actions = tf.placeholder(tf.float32, [None, 1])\n","        \n","        self.rewards = tf.placeholder(tf.float32, [None, 1])\n","        \n","        self.sess = tf.InteractiveSession()\n","                \n","        self.checkpoint_file = os.path.join(checkpoints_dir, 'policy_network.ckpt')\n","        \n","        # input layer\n","        self.input_layer = tf.reshape(self.observations, [tf.shape(self.observations)[0],80,80,1])\n","\n","        # cnn layer1\n","        self.c1 = tf.layers.conv2d(inputs=self.input_layer,\n","                             filters = 5,\n","                             activation = tf.nn.relu,\n","                             strides = [1, 1], \n","                             padding = 'SAME',\n","                             kernel_size = [3, 3])\n","        \n","        # pool layer1\n","        self.p1 = tf.layers.max_pooling2d(inputs=self.c1, pool_size=[2, 2], strides=2)\n","        \n","        # cnn layer2\n","        self.c2 = tf.layers.conv2d(inputs=self.p1,\n","                             filters = 5,\n","                             activation = tf.nn.relu,\n","                             strides = [1, 1], \n","                             padding = 'SAME',\n","                             kernel_size = [3, 3])\n","        \n","        # pool layer2\n","        self.p2 = tf.layers.max_pooling2d(inputs=self.c2, pool_size=[2, 2], strides=2)\n","        \n","        \n","        # dense layer\n","        self.flat = tf.reshape(self.p2, [-1, 20 * 20 * 5])\n","        self.dense = tf.layers.dense(self.flat,\n","                            units=20,\n","                            activation=tf.nn.relu)\n","        \n","        # last layer probability\n","        self.up_probability = tf.layers.dense(\n","            self.dense,\n","            units=1,\n","            activation=tf.sigmoid,\n","            kernel_initializer=tf.contrib.layers.xavier_initializer())\n","\n","\n","        self.loss = tf.losses.log_loss(\n","            labels=self.sampled_actions,\n","            predictions=self.up_probability,\n","            weights=self.rewards)\n","        \n","        self.train_op = tf.train.AdamOptimizer(self.learning_rate).minimize(self.loss)\n","\n","        self.saver = tf.train.Saver()\n","        \n","        tf.global_variables_initializer().run()\n","        \n","\n","    def load_checkpoint(self):\n","        print(\"Loading checkpoint...\")\n","        self.saver.restore(self.sess, self.checkpoint_file)\n","\n","    def save_checkpoint(self):\n","        print(\"Saving checkpoint...\")\n","        self.saver.save(self.sess, self.checkpoint_file)\n","\n","    def get_up_probability(self, observations):\n","        up_probability = self.sess.run(\n","            self.up_probability,\n","            feed_dict={self.observations: observations.reshape([1, -1])})\n","        return up_probability\n","\n","    def train(self, state_action_reward_tuples, episode_n):\n","        print(\"Training with %d (state, action, reward) tuples\" %\n","              len(state_action_reward_tuples))\n","\n","        states, actions, _rewards = zip(*state_action_reward_tuples)\n","        states = np.vstack(states)\n","        actions = np.vstack(actions)\n","        _rewards = np.vstack(_rewards)\n","\n","        feed_dict = {\n","            self.observations: states,\n","            self.sampled_actions: actions,\n","            self.rewards: _rewards\n","        }\n","        self.sess.run(self.train_op, feed_dict)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"rUnZqdHoShZa","colab_type":"text"},"cell_type":"markdown","source":["## Set necessary variables and define functions for dataframe processing and discount reward calculation"]},{"metadata":{"id":"xCA2mF3WS72G","colab_type":"code","colab":{}},"cell_type":"code","source":["learning_rate=0.0001\n","batch_size_episodes=1 # how many rounds we play before updating the weights of network\n","checkpoint_every_n_episodes=10\n","load_checkpoint = True\n","discount_factor=0.99\n","render=False\n","\n","# Action values to send to gym environment to move paddle up/down\n","UP_ACTION = 2\n","DOWN_ACTION = 3\n","# Mapping from action values to outputs from the policy network\n","action_dict = {DOWN_ACTION: 0, UP_ACTION: 1}\n","\n","\n","# From Andrej's code\n","def prepro(I):\n","    \"\"\" prepro 210x160x3 uint8 frame into 6400 (80x80) 1D float vector \"\"\"\n","    I = I[35:195]  # crop\n","    I = I[::2, ::2, 0]  # downsample by factor of 2\n","    I[I == 144] = 0  # erase background (background type 1)\n","    I[I == 109] = 0  # erase background (background type 2)\n","    I[I != 0] = 1  # everything else (paddles, ball) just set to 1\n","    return I.astype(np.float).ravel()\n","\n","# From Andrej's code\n","def discount_rewards(r, discount_factor):\n","    \"\"\" take 1D float array of rewards and compute discounted reward \"\"\"\n","    discounted_r = np.zeros_like(r)\n","    running_add = 0\n","    for t in reversed(range(0, len(r))):\n","      if r[t] != 0: running_add = 0 # reset the sum, since this was a game boundary (pong specific!)\n","      running_add = running_add * discount_factor + r[t]\n","      discounted_r[t] = running_add\n","    return discounted_r\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"BJRDVVkcTAj8","colab_type":"text"},"cell_type":"markdown","source":["## Run it"]},{"metadata":{"id":"lXtCMwKDOsYw","colab_type":"code","colab":{}},"cell_type":"code","source":["env = gym.make('Pong-v0')\n","observation = env.reset()\n","observation = prepro(observation)\n","last_observation = None\n","\n","\n","network = Network(learning_rate, checkpoints_dir=LOG_DIR)\n","if load_checkpoint:\n","    network.load_checkpoint()\n","\n","batch_state_action_reward_tuples = []\n","smoothed_reward = None\n","episode_n = 1\n","round_n = 1    \n","n_steps = 0\n","episode_reward_sum = 0\n","ep_rewards = [] # for plot\n","ep_smooth_rewards = [] # for plot\n","\n","while True:\n","\n","    \n","    if render: env.render()\n","    \n","    observation_delta = observation - last_observation if last_observation is not None else observation\n","    last_observation = observation\n","    \n","    up_probability = network.get_up_probability(observation_delta)[0]\n","    action = UP_ACTION if np.random.uniform() < up_probability else DOWN_ACTION\n","\n","    observation, reward, episode_done, _ = env.step(action)\n","    observation = prepro(observation)\n","    episode_reward_sum += reward\n","    n_steps += 1\n","\n","    tup = (observation_delta, action_dict[action], reward)\n","    batch_state_action_reward_tuples.append(tup)\n","\n","    if reward != 0:\n","        if reward == -1:\n","            print(\"Round %d: %d time steps; lost...\" % (round_n, n_steps))\n","        elif reward == +1:\n","            print(\"Round %d: %d time steps; won!\" % (round_n, n_steps))\n","        round_n += 1\n","        n_steps = 0\n","\n","    if episode_done:\n","        print(\"Episode %d finished after %d rounds\" % (episode_n, round_n-1))\n","        round_n = 1\n","\n","        # exponentially smoothed version of reward\n","        if smoothed_reward is None:\n","            smoothed_reward = episode_reward_sum\n","        else:\n","            smoothed_reward = smoothed_reward * 0.99 + episode_reward_sum * 0.01\n","        print(\"Reward total was %.3f; discounted moving average of reward is %.3f\" \\\n","            % (episode_reward_sum, smoothed_reward))\n","\n","        ep_rewards.append(episode_reward_sum)\n","        ep_smooth_rewards.append(smoothed_reward)\n","        \n","        episode_reward_sum = 0\n","        \n","        if smoothed_reward > 10.0: break\n","\n","        if episode_n % batch_size_episodes == 0:\n","            states, actions, rewards = zip(*batch_state_action_reward_tuples)\n","            rewards = discount_rewards(rewards, discount_factor)\n","            rewards -= np.mean(rewards)\n","            rewards /= np.std(rewards)\n","            batch_state_action_reward_tuples = list(zip(states, actions, rewards))\n","            network.train(batch_state_action_reward_tuples, episode_n)\n","            batch_state_action_reward_tuples = []\n","\n","        if episode_n % checkpoint_every_n_episodes == 0:\n","            network.save_checkpoint()\n","            load_checkpoint = True\n","\n","        episode_n += 1\n","        observation = env.reset()\n","        observation = prepro(observation)\n","        last_observation = None\n","        \n","\n","import pandas as pd\n","df = pd.DataFrame(ep_rewards, columns=[\"ep_rewards\"])\n","df.to_csv('ep_rewards.csv', index=False)\n","\n","df = pd.DataFrame(ep_smooth_rewards, columns=[\"ep_smooth_rewards\"])\n","df.to_csv('ep_smooth_rewards.csv', index=False)\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"LrQWMJsOR3By","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}